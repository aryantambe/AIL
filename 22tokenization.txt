import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from textblob import TextBlob
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

def process_text(filename):
    # Read file
    with open(filename, 'r') as file:
        text = file.read()
    
    # a. Clean text
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    print("cleaned text")
    print(text)
    # b. Convert to lowercase
    text = text.lower()
    print("lower case")
    print(text)
    # c. Tokenization
    tokens = word_tokenize(text)
    # d. Remove stop words
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]
    
    # e. Correct spelling
    corrected_tokens = [str(TextBlob(token).correct()) for token in tokens]
    
    return corrected_tokens

def main():
    filename = 'file1.txt'  
    result = process_text(filename)
    print("Processed Tokens:", result)

if __name__ == "__main__":
    main()
