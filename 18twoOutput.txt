mport numpy as np

# Function to initialize random weights and biases
def initialize_weights_and_biases(input_size, hidden_layer_size, output_size):
    # Initialize weights with random values
    W1 = np.random.randn(input_size, hidden_layer_size)  # Weights for input to hidden layer
    W2 = np.random.randn(hidden_layer_size, output_size)  # Weights for hidden layer to output layer
    
    # Initialize biases with random values
    b1 = np.random.randn(hidden_layer_size)  # Bias for hidden layer
    b2 = np.random.randn(output_size)  # Bias for output layer
    
    return W1, W2, b1, b2

# Function to perform forward pass (no activation function)
def forward_pass(X, W1, W2, b1, b2):
    # First hidden layer
    Z1 = np.dot(X, W1) + b1
    # Output layer
    Z2 = np.dot(Z1, W2) + b2
    return Z1, Z2

# Function to perform training (using random input and output data)
def train_mlp(input_size, hidden_layer_size, output_size, steps=1000):
    # Initialize random weights and biases
    W1, W2, b1, b2 = initialize_weights_and_biases(input_size, hidden_layer_size, output_size)
    
    # Random binary inputs and outputs (for simplicity)
    X = np.random.randint(0, 2, (1, input_size))  # 1 sample with 4 binary inputs
    Y = np.random.randint(0, 2, (1, output_size))  # 2 binary outputs
    
    for step in range(steps):
        # Forward pass
        Z1, Z2 = forward_pass(X, W1, W2, b1, b2)
        
        # Displaying intermediate results every 100 steps
        if step % 100 == 0:
            print(f"Step {step}:")
            print(f"Input (X): {X}")
            print(f"Output (Y): {Y}")
            print(f"Hidden Layer Output (Z1): {Z1}")
            print(f"Output Layer (Z2): {Z2}")
            print(f"Weights (W1): {W1}")
            print(f"Weights (W2): {W2}")
            print(f"Biases (b1): {b1}")
            print(f"Biases (b2): {b2}")
            print()
        
    # Return final weight matrices, biases, and steps
    return W1, W2, b1, b2, steps

# Example of usage
input_size = 4  # Number of binary inputs
hidden_layer_size = 5  # Size of hidden layer
output_size = 2  # Number of binary outputs

# Train the MLP and display the results
W1, W2, b1, b2, steps = train_mlp(input_size, hidden_layer_size, output_size, steps=1000)

print(f"Final Weights (W1):\n{W1}")
print(f"Final Weights (W2):\n{W2}")
print(f"Final Biases (b1):\n{b1}")
print(f"Final Biases (b2):\n{b2}")
print(f"Total Steps: {steps}")